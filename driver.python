from __future__ import division
import random
import sys
from contextlib import contextmanager
from datetime import datetime
from math import floor
from itertools import product
from multiprocessing import freeze_support

from pythonping import ping
import multiprocessing as mp
import os
from dataclasses import dataclass
from icmplib import ping


class Counter(object):
	def __init__(self):
		self.val = mp.Value('i', 0)

	def increment(self, n=1):
		with self.val.get_lock():
			self.val.value += n

	@property
	def value(self):
		return self.val.value


class ProgressBar:
	curPercent: float
	barLength: int
	arrowBody: str
	arrowHead: str
	descriptionText: str

	def __init__(self, title='Progress', bar_length=20, arrow_body='-', arrow_head='>'):
		self.barLength = bar_length
		self.arrowBody = arrow_body
		self.arrowHead = arrow_head
		self.curPercent = 0.0
		self.descriptionText = title

	def update_print(self, new_percent):
		self.update(new_percent)
		self.print()

	def change_text(self, new_text):
		self.descriptionText = new_text
		self.print()

	def update(self, current_percent):
		self.curPercent = current_percent

	def print(self):
		arrow = (self.arrowBody * int(floor(self.curPercent * self.barLength) - 1)) + self.arrowHead
		spaces = ' ' * (self.barLength - len(arrow))
		sys.stdout.write("\r{0}: [{1}] {2}%".format(self.descriptionText, arrow + spaces, int(floor(self.curPercent * 100))))
		sys.stdout.flush()

@dataclass
class PingInfoBundle:
	host: str
	bytes: int
	# repeating_count: int


@dataclass
class TestResult:
	host: str
	packets_sent: int
	packets_received: int
	min_rtt: int
	avg_rtt: int
	max_rtt: int
	packet_loss: float
	succ: bool


# tunable
num_producer = 5
packet_group_count = 1
repeat_everything_count = 2
ping_wait_time = 0.050 #s
ping_timeout = 0.100 #s
payload_size_min = 50
payload_size_max = 75
payload_size_step = 25
output_update_period = 16
sites_to_ping = ['1.1.1.1', '4.2.2.1', '192.0.43.10', 'google.com', 'stackoverflow.com', 'reddit.com', 'bbc.co.uk']


def make_args(host_list, bytes, repeat_count):
	extended_host = [None] * (len(host_list) * repeat_count)
	for i in range(repeat_count):
		for index, item in enumerate(host_list):
			extended_host[index + (len(host_list) * i)] = host_list[index]
	random.shuffle(extended_host)
	return product(extended_host, bytes)


def standalone(commands, commands_mutex, results, res_mutex, my_counter):
	my_pid = os.getpid()
	while True:
		commands_mutex.acquire()
		if not commands.empty():
			cur_res = commands.get()
			print(f'current thing: {cur_res}')
		else:
			break
		commands_mutex.release()
		ping_result = ping(cur_res.host, count=packet_group_count, interval=ping_wait_time, timeout=ping_timeout, id=my_pid, source=None, privileged=True, payload_size=cur_res.bytes)
		res_mutex.acquire()
		results.put(ping_result)
		my_counter.increment()
		res_mutex.release()
		print("loop/n", flush=True)
	print("EXITING/n", flush=True)
	return 0


def pool_worker(host, byte_count):
	sys.stdout.write("bruh")
	sys.stdout.flush()
	print('START')
	my_pid = os.getpid()
	print(f'hello from PID{my_pid}')
	temp = ping(host, count=packet_group_count, interval=ping_wait_time, timeout=ping_timeout, id=my_pid, source=None, privileged=True, payload_size=byte_count)
	return temp


@contextmanager
def poolcontext(*args, **kwargs):
	myPool = mp.Pool(*args, **kwargs)
	yield myPool
	myPool.terminate()


if __name__ == '__main__':
	mp.set_start_method('spawn')
	freeze_support()
	output_q = mp.Queue()
	# output_mutex = mp.Lock()
	counter = Counter()

#	sizes_to_run = [None] * len(range(payload_size_min, payload_size_max + payload_size_step, payload_size_step))
#	for i in range(0, len(range(payload_size_min, payload_size_max + payload_size_step, payload_size_step))):
#		sizes_to_run[i] = payload_size_min + (i * payload_size_step)
#	site_id_list = [None] * len(range(0, len(sites_to_ping)))

	total_work_count = len(range(payload_size_min, payload_size_max + payload_size_step, payload_size_step)) * repeat_everything_count * len(sites_to_ping)

	list_of_packet_sizes = [None] * len(range(payload_size_min, payload_size_max + payload_size_step, payload_size_step))
	for index, size in enumerate(range(payload_size_min, payload_size_max + payload_size_step, payload_size_step)):
		list_of_packet_sizes[index] = size
	work_list = [None] * total_work_count
	work_list = list(product(sites_to_ping, range(payload_size_min, payload_size_max + payload_size_step, payload_size_step)))

	with poolcontext(processes=2) as pool:

		result = pool.starmap_async(pool_worker, make_args(sites_to_ping, list_of_packet_sizes, repeat_everything_count))
		for item in result.get():
			print(f'address: {item.address}')
			print(f'min_rtt: {item.min_rtt}')
			print(f'avg_rtt: {item.avg_rtt}')
			print(f'max_rtt: {item.max_rtt}')
			print(f'packets_sent: {item.packets_sent}')
			print(f'packets_received: {item.packets_received}')
			print(f'packet_loss: {item.packet_loss}')
			print(f'is_alive: {item.is_alive}')
	'''	results = pool.starmap_async(merge_names, product(names, repeat=2))
		print(results.get(timeout=5))
	def log_result(result):
			print("Succesfully get callback! With result: ", result.get())
		pool.starmap_async(pool_worker, product(sites_to_ping, range(payload_size_min, payload_size_max + payload_size_step, payload_size_step)), callback=log_result)
		print('after')
		# tqdm.write('scheduled')


		# multiple_results = [pool.apply_async(pool_worker, job) for job in work_list]
		# for _ in tqdm.tqdm(pool.starmap_async(pool_worker, (output_q, work_list), total=total_work_count)):
		#	print("Waiting for", remaining, "tasks to complete...")
		#	pass
		# for i in enumerate(pool.starmap_async(pool_worker, (output_q, work_list)), 1):
		# 	sys.stderr.write('\rdone {0:%}'.format(i / total_work_count))

		results = pool.imap_unordered(pool_worker, work_list)
		pool.close()  # No more work
		while True:
			if results.ready():
				break
			remaining = results._number_left
			print("Waiting for", remaining, "tasks to complete...")'''


	# while not output_q.empty():
	# 	print(output_q.get())
